<<<<<<< HEAD
from fastapi import APIRouter, HTTPException, Query, Depends, Body
from typing import Optional, List, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, func
from datetime import datetime
from uuid import UUID

from app.core.database import get_db
from app.models.marine_data import (
    UnifiedMarineData, OceanographicData, FisheriesData, 
    TaxonomicData, MolecularData
)
from app.services.ai_service import ai_service
import logging

logger = logging.getLogger(__name__)
router = APIRouter()

@router.get("/correlations/cross-domain")
async def analyze_cross_domain_correlations(
    region: Optional[str] = Query(None),
    latitude_min: Optional[float] = Query(None),
    latitude_max: Optional[float] = Query(None),
    longitude_min: Optional[float] = Query(None),
    longitude_max: Optional[float] = Query(None),
    time_window_days: int = Query(30, description="Time window for correlation analysis"),
    db: Session = Depends(get_db)
):
    """Analyze correlations between oceanographic conditions and biodiversity patterns"""
    try:
        # Build base query filters
        base_filters = []
        
        if region:
            base_filters.append(UnifiedMarineData.region.ilike(f"%{region}%"))
        if latitude_min is not None:
            base_filters.append(UnifiedMarineData.latitude >= latitude_min)
        if latitude_max is not None:
            base_filters.append(UnifiedMarineData.latitude <= latitude_max)
        if longitude_min is not None:
            base_filters.append(UnifiedMarineData.longitude >= longitude_min)
        if longitude_max is not None:
            base_filters.append(UnifiedMarineData.longitude <= longitude_max)
        
        # Get oceanographic data
        ocean_query = db.query(UnifiedMarineData, OceanographicData).join(
            OceanographicData, UnifiedMarineData.id == OceanographicData.unified_data_id
        ).filter(and_(*base_filters))
        
        ocean_data = ocean_query.all()
        
        # Get biodiversity data (fisheries + molecular)
        bio_query = db.query(UnifiedMarineData).filter(
            and_(*base_filters,
                 or_(UnifiedMarineData.data_type == "fisheries",
                     UnifiedMarineData.data_type == "molecular"),
                 UnifiedMarineData.scientific_name.is_not(None))
        )
        
        bio_data = bio_query.all()
        
        # Spatial-temporal correlation analysis
        correlations = []
        
        for ocean_record, ocean_details in ocean_data:
            # Find nearby biodiversity records within time window
            nearby_bio = []
            
            for bio_record in bio_data:
                # Check spatial proximity (within 0.1 degrees)
                ocean_lat = getattr(ocean_record, 'latitude', None)
                ocean_lon = getattr(ocean_record, 'longitude', None)
                bio_lat = getattr(bio_record, 'latitude', None)
                bio_lon = getattr(bio_record, 'longitude', None)
                
                if (ocean_lat is not None and bio_lat is not None and
                    ocean_lon is not None and bio_lon is not None):
                    lat_diff = abs(float(ocean_lat) - float(bio_lat))
                    lon_diff = abs(float(ocean_lon) - float(bio_lon))
                    
                    if lat_diff <= 0.1 and lon_diff <= 0.1:
                        # Check temporal proximity
                        ocean_date = getattr(ocean_record, 'collection_date', None)
                        bio_date = getattr(bio_record, 'collection_date', None)
                        
                        if (ocean_date is not None and bio_date is not None):
                            time_diff = abs((ocean_date - bio_date).days)
                            if time_diff <= time_window_days:
                                nearby_bio.append(bio_record)
            
            if nearby_bio:
                correlation_record = {
                    "oceanographic_data": {
                        "location": {
                            "latitude": ocean_record.latitude,
                            "longitude": ocean_record.longitude,
                            "region": ocean_record.region
                        },
                        "date": ocean_record.collection_date.isoformat() if ocean_record.collection_date else None,
                        "temperature": ocean_details.temperature,
                        "salinity": ocean_details.salinity,
                        "ph": ocean_details.ph,
                        "dissolved_oxygen": ocean_details.dissolved_oxygen
                    },
                    "biodiversity_data": {
                        "species_count": len(set(b.scientific_name for b in nearby_bio)),
                        "total_observations": len(nearby_bio),
                        "species_list": list(set(b.scientific_name for b in nearby_bio)),
                        "data_types": list(set(b.data_type for b in nearby_bio))
                    }
                }
                correlations.append(correlation_record)
        
        # Generate correlation insights
        correlation_summary = {
            "total_correlation_points": len(correlations),
            "temperature_ranges": {},
            "biodiversity_patterns": {},
            "environmental_factors": {}
        }
        
        if correlations:
            # Temperature vs biodiversity analysis
            temp_data = [c["oceanographic_data"]["temperature"] for c in correlations 
                        if c["oceanographic_data"]["temperature"]]
            species_counts = [c["biodiversity_data"]["species_count"] for c in correlations]
            
            if temp_data and species_counts:
                correlation_summary["temperature_ranges"] = {
                    "min": min(temp_data),
                    "max": max(temp_data),
                    "mean": sum(temp_data) / len(temp_data)
                }
                
                correlation_summary["biodiversity_patterns"] = {
                    "avg_species_per_location": sum(species_counts) / len(species_counts),
                    "max_species_diversity": max(species_counts),
                    "total_unique_species": len(set(
                        species for c in correlations 
                        for species in c["biodiversity_data"]["species_list"]
                    ))
                }
        
        # Generate AI analysis
        ai_analysis = await ai_service.analyze_marine_data({
            "analysis_type": "cross_domain_correlation",
            "region": region,
            "correlation_summary": correlation_summary,
            "sample_correlations": correlations[:10],  # Sample for AI analysis
            "time_window_days": time_window_days
        })
        
        return {
            "analysis_parameters": {
                "region": region,
                "spatial_bounds": {
                    "latitude_range": [latitude_min, latitude_max],
                    "longitude_range": [longitude_min, longitude_max]
                },
                "time_window_days": time_window_days
            },
            "correlation_summary": correlation_summary,
            "correlations": correlations[:100],  # Limit response size
            "ai_analysis": ai_analysis
        }
        
    except Exception as e:
        logger.error(f"Failed to analyze cross-domain correlations: {e}")
        raise HTTPException(status_code=500, detail=f"Correlation analysis failed: {str(e)}")

@router.get("/integration/unified-view")
async def get_unified_data_view(
    location_id: Optional[str] = Query(None, description="Specific location identifier"),
    latitude: Optional[float] = Query(None),
    longitude: Optional[float] = Query(None),
    radius_km: float = Query(10.0, description="Search radius in kilometers"),
    include_all_data_types: bool = Query(True),
    db: Session = Depends(get_db)
):
    """Get unified view of all data types for a specific location"""
    try:
        # Build spatial query
        if latitude is not None and longitude is not None:
            # Convert km to degrees (approximate)
            radius_deg = radius_km / 111.0  # 1 degree â‰ˆ 111 km
            
            base_query = db.query(UnifiedMarineData).filter(
                UnifiedMarineData.latitude.between(latitude - radius_deg, latitude + radius_deg),
                UnifiedMarineData.longitude.between(longitude - radius_deg, longitude + radius_deg)
            )
        elif location_id:
            base_query = db.query(UnifiedMarineData).filter(
                or_(UnifiedMarineData.location_name.ilike(f"%{location_id}%"),
                    UnifiedMarineData.region.ilike(f"%{location_id}%"))
            )
        else:
            raise HTTPException(status_code=400, detail="Must provide either coordinates or location_id")
        
        # Get all unified data
        all_data = base_query.all()
        
        # Organize by data type
        unified_view = {
            "location_info": {
                "latitude": latitude,
                "longitude": longitude,
                "location_id": location_id,
                "radius_km": radius_km
            },
            "data_summary": {
                "total_records": len(all_data),
                "data_types": {},
                "date_range": {"earliest": None, "latest": None},
                "spatial_coverage": {"lat_range": [], "lon_range": []}
            },
            "integrated_data": {
                "oceanographic": [],
                "fisheries": [],
                "taxonomic": [],
                "molecular": []
            }
        }
        
        # Process each data type with safe attribute access
        for record in all_data:
            try:
                data_type = getattr(record, 'data_type', 'unknown')
                
                # Update summary
                unified_view["data_summary"]["data_types"][data_type] = \
                    unified_view["data_summary"]["data_types"].get(data_type, 0) + 1
                
                # Update date range with safe attribute access
                collection_date = getattr(record, 'collection_date', None)
                if collection_date is not None:
                    earliest = unified_view["data_summary"]["date_range"]["earliest"]
                    latest = unified_view["data_summary"]["date_range"]["latest"]
                    
                    if not earliest or collection_date < earliest:
                        unified_view["data_summary"]["date_range"]["earliest"] = collection_date
                    
                    if not latest or collection_date > latest:
                        unified_view["data_summary"]["date_range"]["latest"] = collection_date
                
                # Basic record info with safe attribute access
                base_info = {
                    "id": str(getattr(record, 'id', 'unknown')),
                    "collection_date": collection_date.isoformat() if collection_date is not None else None,
                    "latitude": getattr(record, 'latitude', None),
                    "longitude": getattr(record, 'longitude', None),
                    "depth": getattr(record, 'depth', None),
                    "region": getattr(record, 'region', None),
                    "scientific_name": getattr(record, 'scientific_name', None),
                    "source_dataset": getattr(record, 'source_dataset', None),
                    "data_quality_score": getattr(record, 'data_quality_score', None)
                }
                
                # Add to appropriate category
                if data_type in unified_view["integrated_data"]:
                    unified_view["integrated_data"][data_type].append(base_info)
                    
            except Exception as e:
                logger.warning(f"Error processing record {getattr(record, 'id', 'unknown')}: {e}")
                continue
        
        # Calculate spatial coverage with safe attribute access
        if all_data:
            lats = []
            lons = []
            
            for r in all_data:
                lat = getattr(r, 'latitude', None)
                lon = getattr(r, 'longitude', None)
                if lat is not None:
                    lats.append(float(lat))
                if lon is not None:
                    lons.append(float(lon))
            
            if lats:
                unified_view["data_summary"]["spatial_coverage"]["lat_range"] = [min(lats), max(lats)]
            if lons:
                unified_view["data_summary"]["spatial_coverage"]["lon_range"] = [min(lons), max(lons)]
        
        # Convert dates to ISO format for JSON serialization
        if unified_view["data_summary"]["date_range"]["earliest"]:
            unified_view["data_summary"]["date_range"]["earliest"] = \
                unified_view["data_summary"]["date_range"]["earliest"].isoformat()
        if unified_view["data_summary"]["date_range"]["latest"]:
            unified_view["data_summary"]["date_range"]["latest"] = \
                unified_view["data_summary"]["date_range"]["latest"].isoformat()
        
        # Generate AI analysis
        ai_analysis = await ai_service.analyze_marine_data({
            "analysis_type": "unified_location_view",
            "location_info": unified_view["location_info"],
            "data_summary": unified_view["data_summary"],
            "sample_data": {k: v[:5] for k, v in unified_view["integrated_data"].items()}
        })
        
        return {
            "unified_view": unified_view,
            "ai_analysis": ai_analysis,
            "data_completeness": {
                "has_oceanographic": len(unified_view["integrated_data"]["oceanographic"]) > 0,
                "has_fisheries": len(unified_view["integrated_data"]["fisheries"]) > 0,
                "has_taxonomic": len(unified_view["integrated_data"]["taxonomic"]) > 0,
                "has_molecular": len(unified_view["integrated_data"]["molecular"]) > 0,
                "completeness_score": sum([
                    len(unified_view["integrated_data"]["oceanographic"]) > 0,
                    len(unified_view["integrated_data"]["fisheries"]) > 0,
                    len(unified_view["integrated_data"]["taxonomic"]) > 0,
                    len(unified_view["integrated_data"]["molecular"]) > 0
                ]) / 4.0
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get unified data view: {e}")
        raise HTTPException(status_code=500, detail=f"Unified view failed: {str(e)}")

@router.post("/integration/data-fusion")
async def perform_data_fusion(
    fusion_config: Dict[str, Any] = Body(..., description="Configuration for data fusion"),
    db: Session = Depends(get_db)
):
    """Perform advanced data fusion across multiple marine data types"""
    try:
        fusion_type = fusion_config.get("fusion_type", "spatial_temporal")
        target_species = fusion_config.get("target_species")
        region = fusion_config.get("region")
        time_range = fusion_config.get("time_range", {})
        
        # Query relevant data based on fusion configuration
        query_filters = []
        
        if region:
            query_filters.append(UnifiedMarineData.region.ilike(f"%{region}%"))
        
        if target_species:
            query_filters.append(UnifiedMarineData.scientific_name.ilike(f"%{target_species}%"))
        
        if time_range.get("start_date"):
            start_date = datetime.fromisoformat(time_range["start_date"])
            query_filters.append(UnifiedMarineData.collection_date >= start_date)
        
        if time_range.get("end_date"):
            end_date = datetime.fromisoformat(time_range["end_date"])
            query_filters.append(UnifiedMarineData.collection_date <= end_date)
        
        # Get data from all sources
        base_query = db.query(UnifiedMarineData).filter(and_(*query_filters))
        all_records = base_query.all()
        
        # Perform fusion based on type
        fusion_results = {
            "fusion_type": fusion_type,
            "target_species": target_species,
            "region": region,
            "input_records": len(all_records),
            "fused_data": [],
            "fusion_quality": {},
            "insights": []
        }
        
        if fusion_type == "spatial_temporal":
            # Group records by spatial-temporal proximity
            fusion_groups = []
            processed_records = set()
            
            for record in all_records:
                if str(getattr(record, 'id', '')) in processed_records:
                    continue
                
                # Create a fusion group for this record with safe attribute access
                record_lat = getattr(record, 'latitude', None)
                record_lon = getattr(record, 'longitude', None)
                record_date = getattr(record, 'collection_date', None)
                record_data_type = getattr(record, 'data_type', 'unknown')
                
                group = {
                    "center_location": {
                        "latitude": record_lat,
                        "longitude": record_lon,
                        "region": getattr(record, 'region', None)
                    },
                    "time_center": record_date,
                    "records": [record],
                    "data_types": {record_data_type}
                }
                
                # Find nearby records in space and time with safe attribute access
                for other_record in all_records:
                    other_id = str(getattr(other_record, 'id', ''))
                    record_id = str(getattr(record, 'id', ''))
                    
                    if (other_id != record_id and other_id not in processed_records):
                        
                        # Check spatial proximity (within 0.05 degrees)
                        other_lat = getattr(other_record, 'latitude', None)
                        other_lon = getattr(other_record, 'longitude', None)
                        
                        if (record_lat is not None and other_lat is not None and
                            record_lon is not None and other_lon is not None):
                            lat_diff = abs(float(record_lat) - float(other_lat))
                            lon_diff = abs(float(record_lon) - float(other_lon))
                            
                            # Check temporal proximity (within 7 days)
                            time_diff = 999
                            other_date = getattr(other_record, 'collection_date', None)
                            
                            if (record_date is not None and other_date is not None):
                                time_diff = abs((record_date - other_date).days)
                            
                            if lat_diff <= 0.05 and lon_diff <= 0.05 and time_diff <= 7:
                                group["records"].append(other_record)
                                group["data_types"].add(getattr(other_record, 'data_type', 'unknown'))
                                processed_records.add(other_id)
                
                processed_records.add(str(getattr(record, 'id', '')))
                
                # Only include groups with multiple data types
                if len(group["data_types"]) > 1:
                    fusion_groups.append(group)
            
            # Convert fusion groups to results
            for group in fusion_groups:
                fused_record = {
                    "fusion_id": f"fusion_{len(fusion_results['fused_data'])}",
                    "location": group["center_location"],
                    "time_center": group["time_center"].isoformat() if group["time_center"] else None,
                    "data_types_included": list(group["data_types"]),
                    "record_count": len(group["records"]),
                    "species_observed": list(set(r.scientific_name for r in group["records"] if r.scientific_name)),
                    "fusion_confidence": len(group["data_types"]) / 4.0  # Max 4 data types
                }
                fusion_results["fused_data"].append(fused_record)
        
        # Calculate fusion quality metrics
        if fusion_results["fused_data"]:
            confidences = [f["fusion_confidence"] for f in fusion_results["fused_data"]]
            fusion_results["fusion_quality"] = {
                "average_confidence": sum(confidences) / len(confidences),
                "high_quality_fusions": len([c for c in confidences if c > 0.7]),
                "multi_domain_coverage": len([f for f in fusion_results["fused_data"] if len(f["data_types_included"]) >= 3])
            }
        
        # Generate AI analysis
        ai_analysis = await ai_service.analyze_marine_data({
            "analysis_type": "data_fusion",
            "fusion_config": fusion_config,
            "fusion_results": fusion_results,
            "sample_fused_data": fusion_results["fused_data"][:5]
        })
        
        fusion_results["ai_analysis"] = ai_analysis
        
        return fusion_results
        
    except Exception as e:
        logger.error(f"Failed to perform data fusion: {e}")
        raise HTTPException(status_code=500, detail=f"Data fusion failed: {str(e)}")

@router.get("/knowledge/synthesis")
async def synthesize_knowledge(
    research_question: str = Query(..., description="Research question to synthesize knowledge for"),
    include_data_types: List[str] = Query(["oceanographic", "fisheries", "taxonomic", "molecular"]),
    region: Optional[str] = Query(None),
    db: Session = Depends(get_db)
):
    """Synthesize knowledge across all data types to answer research questions"""
    try:
        # Query relevant data based on research question keywords
        keywords = research_question.lower().split()
        
        # Build query based on keywords and data types
        query = db.query(UnifiedMarineData).filter(
            UnifiedMarineData.data_type.in_(include_data_types)
        )
        
        if region:
            query = query.filter(UnifiedMarineData.region.ilike(f"%{region}%"))
        
        # Filter by research question keywords (simple implementation)
        for keyword in keywords:
            if keyword in ["temperature", "temp", "thermal"]:
                query = query.filter(or_(
                    UnifiedMarineData.data_type == "oceanographic",
                    UnifiedMarineData.tags.any(keyword)
                ))
            elif keyword in ["species", "biodiversity", "diversity"]:
                query = query.filter(or_(
                    UnifiedMarineData.data_type.in_(["fisheries", "molecular", "taxonomic"]),
                    UnifiedMarineData.scientific_name.is_not(None)
                ))
        
        relevant_data = query.limit(1000).all()
        
        # Organize data for synthesis
        synthesis_data = {
            "research_question": research_question,
            "data_sources": {
                "oceanographic": [],
                "fisheries": [],
                "taxonomic": [],
                "molecular": []
            },
            "key_findings": [],
            "data_summary": {},
            "knowledge_gaps": []
        }
        
        # Categorize data
        for record in relevant_data:
            if record.data_type in synthesis_data["data_sources"]:
                collection_date_attr = getattr(record, 'collection_date', None)
                synthesis_data["data_sources"][record.data_type].append({
                    "id": str(record.id),
                    "scientific_name": record.scientific_name,
                    "location": record.region,
                    "date": collection_date_attr.isoformat() if collection_date_attr is not None else None,
                    "quality_score": record.data_quality_score
                })
        
        # Generate data summary
        for data_type, records in synthesis_data["data_sources"].items():
            synthesis_data["data_summary"][data_type] = {
                "record_count": len(records),
                "date_range": {
                    "earliest": min([r["date"] for r in records if r["date"]], default=None),
                    "latest": max([r["date"] for r in records if r["date"]], default=None)
                },
                "species_count": len(set(r["scientific_name"] for r in records if r["scientific_name"])),
                "locations": list(set(r["location"] for r in records if r["location"]))
            }
        
        # Generate comprehensive AI synthesis
        ai_synthesis = await ai_service.analyze_marine_data({
            "analysis_type": "knowledge_synthesis",
            "research_question": research_question,
            "synthesis_data": synthesis_data,
            "region": region,
            "include_data_types": include_data_types
        })
        
        return {
            "research_question": research_question,
            "synthesis_summary": {
                "total_records_analyzed": len(relevant_data),
                "data_types_included": include_data_types,
                "region": region,
                "temporal_coverage": {
                    data_type: summary["date_range"] 
                    for data_type, summary in synthesis_data["data_summary"].items()
                }
            },
            "data_summary": synthesis_data["data_summary"],
            "ai_synthesis": ai_synthesis,
            "recommendations": [
                "Cross-reference findings with additional literature",
                "Consider temporal and spatial limitations of data",
                "Validate conclusions with field observations",
                "Identify areas needing additional data collection"
            ]
        }
        
    except Exception as e:
        logger.error(f"Failed to synthesize knowledge: {e}")
        raise HTTPException(status_code=500, detail=f"Knowledge synthesis failed: {str(e)}")
=======
from fastapi import APIRouter, HTTPException, Query
from typing import Optional, List, Dict, Any
from app.services.external_apis import obis_client, gbif_client
from app.services.llm_service import llm_service
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/integrate/multi-source")
async def integrate_multi_source_data(
    data_sources: Dict[str, Any]
):
    """Integrate data from multiple sources (OBIS, GBIF, oceanographic sensors)"""
    try:
        integrated_results = {
            "integration_timestamp": "2024-01-01T00:00:00Z",
            "sources_processed": list(data_sources.keys()),
            "integrated_data": {}
        }
        
        # Process each data source
        for source_name, source_params in data_sources.items():
            if source_name == "obis" and "species_query" in source_params:
                obis_data = await obis_client.search_species(
                    scientific_name=source_params.get("species_query"),
                    geometry=source_params.get("geometry"),
                    limit=source_params.get("limit", 50)
                )
                integrated_results["integrated_data"]["biodiversity_obis"] = obis_data
            
            elif source_name == "gbif" and "species_query" in source_params:
                gbif_data = await gbif_client.search_occurrences(
                    scientific_name=source_params.get("species_query"),
                    country=source_params.get("country"),
                    limit=source_params.get("limit", 50)
                )
                integrated_results["integrated_data"]["biodiversity_gbif"] = gbif_data
        
        # Generate AI insights on integrated data
        integration_insights = await llm_service.analyze_marine_data(
            integrated_results["integrated_data"],
            "Analyze this integrated marine dataset and identify cross-domain correlations"
        )
        
        return {
            "integration_results": integrated_results,
            "ai_insights": integration_insights,
            "data_quality": {
                "completeness": 0.85,
                "consistency": 0.92,
                "integration_score": 0.78
            }
        }
        
    except Exception as e:
        logger.error(f"Multi-source integration failed: {e}")
        raise HTTPException(status_code=500, detail=f"Data integration failed: {str(e)}")

@router.post("/standardize/biodiversity")
async def standardize_biodiversity_data(
    raw_data: Dict[str, Any],
    target_standard: str = Query("darwin_core", description="Target standard: darwin_core, obis, gbif")
):
    """Standardize biodiversity data according to international standards"""
    try:
        # Simulate data standardization
        if target_standard == "darwin_core":
            standardized_data = {
                "scientificName": raw_data.get("species_name", ""),
                "kingdom": raw_data.get("kingdom", "Animalia"),
                "decimalLatitude": raw_data.get("latitude", ""),
                "decimalLongitude": raw_data.get("longitude", ""),
                "eventDate": raw_data.get("observation_date", ""),
                "basisOfRecord": "HumanObservation"
            }
        else:
            standardized_data = raw_data
        
        return {
            "original_data": raw_data,
            "standardized_data": standardized_data,
            "target_standard": target_standard,
            "compliance_score": 0.92
        }
        
    except Exception as e:
        logger.error(f"Data standardization failed: {e}")
        raise HTTPException(status_code=500, detail=f"Standardization failed: {str(e)}")

@router.get("/pipeline/status")
async def get_pipeline_status():
    """Get status of data ingestion and processing pipelines"""
    try:
        pipeline_status = {
            "data_ingestion": {
                "obis_pipeline": {"status": "running", "records_processed": 1543},
                "gbif_pipeline": {"status": "running", "records_processed": 2876}
            },
            "system_health": {
                "cpu_usage": 0.45,
                "memory_usage": 0.67,
                "active_connections": 15
            }
        }
        
        return {
            "timestamp": "2024-01-01T12:00:00Z",
            "pipeline_status": pipeline_status,
            "overall_health": "healthy"
        }
        
    except Exception as e:
        logger.error(f"Failed to get pipeline status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve pipeline status: {str(e)}")
>>>>>>> 362b52b683dacbc43ff77fceb651bab6d409b1b0
